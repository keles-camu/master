{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso Práctico 4.6: Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "* El LDA, es un ***modelo probabilístico*** que se enmarca dentro de los ***modelos generativos*** ya que trata de describir como se crea un documento. (http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "\n",
    "\n",
    "* El LDA propone que ***un documento se crea mediante la selección de los temas y las palabras de acuerdo a las representaciones probabilísticas del texto natural del documento***.\n",
    "\n",
    "\n",
    "* El LDA calcula **dos matrices de probabilidad P(w|z) y P(z|θ)**, donde:\n",
    "\n",
    "    - **P(w|z)**: es la probabilidad de que dado un tema salga una palabra\n",
    "    - **P(z|θ)**: es la probabilidad de que un documento pertenezca a un tema.\n",
    "    \n",
    "    \n",
    "$$ P(w|\\theta) = \\sum_{z \\in  Z} P(w | z) \\cdot P(z|\\theta) $$\n",
    "\n",
    "\n",
    "* ***NOTA***: *existe otro modelo llamado PLSI (Probabilistic Latent Semantic Index) que se enmarca dentro de los modelos probabilísticos (al igual que el LDA) y es el enfoque probabilístico del LSI (siguiendo una distribución uniforme).*\n",
    "\n",
    "\n",
    "* La representación en \"*Plate Notaticon*\" del LDA es la siguiente:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/018_LDA_Plate_Notation.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "* Siendo\n",
    "\n",
    "    + ***K***: Número de temas. \n",
    "    + ***N***: Número de palabras\n",
    "    + ***M***: Número de documentos: \n",
    "    + ***α***: Parámetro de Dirichlet. Este parámetro es un vector de K componentes que describe el conocimiento a priori que se tiene sobre como los temas se distribuyen en los documentos. \n",
    "        * Pocos temas -> Valor de α pequeño\n",
    "        * Muchos temas  -> Valor de α grande\n",
    "    + ***Β***: Parámetro de Dirichlet. Este parámetro es un vector de N componentes que describe el conocimiento a priori que se tiene sobre como las palabras se distribuyen en cada tema. \n",
    "        * Tema con pocas palabras -> Valor de Β cercano a cero\n",
    "        * Tema con muchas palabras -> Valor de Β cercano a uno\n",
    "    + ***θ***: Distribución de probabilidad de que un documento pertenezca a un tema. \n",
    "    + ***Z***: Distribución de probabilidad de que una palabra pertenezca a un tema. \n",
    "    + ***W***: Identifica todas las palabras en todos los documentos.\n",
    "    + ***φ***: Distribución de probabilidad de que dado un tema salga una palabra.\n",
    "    \n",
    "\n",
    "* ***NOTA***: Si los parámetros ***α y Β*** de la distribución de dirichlet son ***igual a '1'***, el ***LDA se comportaría de la misma manera que el PLSI ya que la distribución de Dirichlet con esos parámetros se comportaría como una distribución uniforme***.\n",
    "\n",
    "\n",
    "* Haciendo una analogía con el LSI, el ***LDA nos tienen que proporcionar***:\n",
    "    - Matriz de probabilidades \"***Temas-Palabras***\": Nos indica la probabilidad de que dado un tema, salga una palabra. \n",
    "    - Matriz de probabilidades \"***Documentos-Temas***\": Nos indica la probabilidad de que un documento pertenezca a un tema.\n",
    "    \n",
    "    \n",
    "* De esta manera podemos ver las relaciones entre palabras y entre documentos.\n",
    "\n",
    "\n",
    "* ***CUIDADO***: *El LDA trabaja con distribuciones de probabilidad que representan la probabilidad de pertenencia de cada palabra o documento a cada tema. Estas distribuciones de probabilidad no tienen que ser tratadas como vectores de factores latentes (como en el LSI) para calcular similaridades entre documentos, ya que al tratarse de distribuciones de probabilidad no hay que aplicar medidas de distancias para calcular similaridades; si no la ***Divergencia de Kullback-Liebler (KL)*** para estudiar las similaridades entre distribuciones de probabilidad.*\n",
    "\n",
    "\n",
    "$$ KL(p ||q) = \\sum_{i}p(i)ln \\frac{p(i)}{q(i)} $$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "# Ejemplo de LDA con Gensim\n",
    "\n",
    "\n",
    "* Veamos a continuación un ejemplo sencillo sobre el siguiente Corpus del cual podemos ver que habla de 3 temas:\n",
    "    - Fútbol\n",
    "    - Política\n",
    "    - Economía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "corpus = [\"balon balon balon futbol futbol liga liga liga ronaldo ronaldo ronaldo ronaldo ronaldo messi\",\n",
    "          \"futbol futbol futbol futbol futbol ronaldo ronaldo ronaldo ronaldo messi messi\",\n",
    "          \"balon balon futbol futbol futbol futbol futbol futbol futbol messi messi messi messi messi\",\n",
    "          \"politica politica politica politica pp pp pp pp pp pp rajoy rajoy rajoy rajoy rajoy\",\n",
    "          \"politica politica politica politica pp pp pp psoe psoe psoe psoe zapatero zapatero zapatero rajoy\",\n",
    "          \"politica politica politica politica psoe psoe psoe psoe psoe psoe zapatero zapatero zapatero zapatero zapatero \",\n",
    "          \"dinero fmi fmi fmi fmi fmi ue ue ue ue pib pib pib ibex ibex\",\n",
    "          \"zapatero rajoy dinero dinero dinero dinero fmi fmi fmi fmi ue ue ue ue pib\",\n",
    "          \"pp psoe zapatero rajoy dinero dinero dinero dinero fmi fmi fmi fmi ue ue ue \",\n",
    "          \"futbol politica pib\",\n",
    "          \"futbol zapatero liga rajoy\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el Diccionario y la Matriz (Bolsa de Palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario:\n",
      "{'balon': 0,\n",
      " 'dinero': 10,\n",
      " 'fmi': 11,\n",
      " 'futbol': 1,\n",
      " 'ibex': 12,\n",
      " 'liga': 2,\n",
      " 'messi': 3,\n",
      " 'pib': 13,\n",
      " 'politica': 5,\n",
      " 'pp': 6,\n",
      " 'psoe': 8,\n",
      " 'rajoy': 7,\n",
      " 'ronaldo': 4,\n",
      " 'ue': 14,\n",
      " 'zapatero': 9}\n",
      "\n",
      "Bolsa de Palabras:\n",
      "[[(0, 3), (1, 2), (2, 3), (3, 1), (4, 5)],\n",
      " [(1, 5), (3, 2), (4, 4)],\n",
      " [(0, 2), (1, 7), (3, 5)],\n",
      " [(5, 4), (6, 6), (7, 5)],\n",
      " [(5, 4), (6, 3), (7, 1), (8, 4), (9, 3)],\n",
      " [(5, 4), (8, 6), (9, 5)],\n",
      " [(10, 1), (11, 5), (12, 2), (13, 3), (14, 4)],\n",
      " [(7, 1), (9, 1), (10, 4), (11, 4), (13, 1), (14, 4)],\n",
      " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 4), (11, 4), (14, 3)],\n",
      " [(1, 1), (5, 1), (13, 1)],\n",
      " [(1, 1), (2, 1), (7, 1), (9, 1)]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "\n",
    "# Tokenizamos\n",
    "documents = [word.split() for word in corpus]\n",
    "\n",
    "# Creamos el diccionario (vocabulario)\n",
    "frequency = defaultdict(int)\n",
    "for doc in documents:\n",
    "    for token in doc:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "documents = [[token for token in doc] for doc in documents]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "print('Diccionario:')\n",
    "pprint(dictionary.token2id)\n",
    "\n",
    "\n",
    "# Creamos la Bolsa de Palabras\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "print('\\nBolsa de Palabras:')\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el Modelo:\n",
    "\n",
    "* Gensim tiene implementado el LDA en la clase ***LdaModel***: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "\n",
    "* Como parámetros relevantes necesita:\n",
    "    1. Corpus\n",
    "    2. Número de Topics\n",
    "    3. Diccionario o Vocabulario del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=3, id2word=dictionary, random_state=168)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de probabilidades \"***Documentos-Temas***\"\n",
    "\n",
    "\n",
    "* Obtenemos la probabilidad de que cada documento pertenezca a uno de los 3 temas de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9554772 , 0.02227203, 0.0222508 ],\n",
       "       [0.9443659 , 0.02783238, 0.02780171],\n",
       "       [0.95548785, 0.02226958, 0.02224256],\n",
       "       [0.02196671, 0.9566049 , 0.02142836],\n",
       "       [0.02129496, 0.95682615, 0.02187886],\n",
       "       [0.02108419, 0.9566616 , 0.02225418],\n",
       "       [0.0209697 , 0.02090397, 0.9581263 ],\n",
       "       [0.0212955 , 0.02190298, 0.9568015 ],\n",
       "       [0.0217593 , 0.03364856, 0.9445922 ],\n",
       "       [0.47187737, 0.22474015, 0.3033825 ],\n",
       "       [0.6306039 , 0.296256  , 0.07314003]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "docs_topics = np.array([[tup[1] for tup in lst] for lst in lda_model[corpus]])\n",
    "docs_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se puede observar que nos devuelve para cada documento la probabilidad de que el documento pertenezca a cada tema y que es un vector de probabilidades ya que la suma de las probabilidades es igual a '1'.\n",
    "\n",
    "\n",
    "* Para ver los factores de cada una de las palabras lo vamos a mostrar de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc 1</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 2</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 3</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 4</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 5</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 6</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 7</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 8</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 9</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 10</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 11</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic 1  Topic 2  Topic 3\n",
       "Doc 1      0.96     0.02     0.02\n",
       "Doc 2      0.94     0.03     0.03\n",
       "Doc 3      0.96     0.02     0.02\n",
       "Doc 4      0.02     0.96     0.02\n",
       "Doc 5      0.02     0.96     0.02\n",
       "Doc 6      0.02     0.96     0.02\n",
       "Doc 7      0.02     0.02     0.96\n",
       "Doc 8      0.02     0.02     0.96\n",
       "Doc 9      0.02     0.03     0.94\n",
       "Doc 10     0.47     0.22     0.30\n",
       "Doc 11     0.63     0.30     0.07"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "index = ['Doc {}'.format(i+1) for i,doc in enumerate(documents)]\n",
    "pd.DataFrame(docs_topics, index=index, columns=['Topic 1', 'Topic 2', 'Topic 3']).head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de probabilidades \"***Temas-Palabras***\"\n",
    "\n",
    "\n",
    "* Obtenemos la probabilidad de que dado uno de los 3 temas aparezca una de las 15 palabras. Al igual que en la matriz anterior la suma de todas las probabilidades de las palabras en un tema tiene que sumar '1'.\n",
    "\n",
    "\n",
    "* A continuación obtenermos la probabilidad de que dado un tema aparezca una palabra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09317508, 0.28258345, 0.07501967, 0.14526479, 0.16276358,\n",
       "        0.04907704, 0.04666422, 0.0684643 , 0.00649099, 0.02347379,\n",
       "        0.00656334, 0.00695538, 0.00597146, 0.02105455, 0.00647843],\n",
       "       [0.01013361, 0.01513246, 0.01031102, 0.01078803, 0.01070127,\n",
       "        0.23358661, 0.19990115, 0.1072669 , 0.18813199, 0.1564047 ,\n",
       "        0.01078731, 0.01139284, 0.00977451, 0.01385712, 0.01183059],\n",
       "       [0.0059508 , 0.00705414, 0.00637555, 0.00625579, 0.00640785,\n",
       "        0.05120316, 0.02275782, 0.03977361, 0.08520272, 0.08768239,\n",
       "        0.15633687, 0.22321236, 0.03918658, 0.07299452, 0.18960586]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_topics = lda_model.get_topics()\n",
    "words_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* De manera más clara los mostramos en una tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balon</th>\n",
       "      <th>futbol</th>\n",
       "      <th>liga</th>\n",
       "      <th>messi</th>\n",
       "      <th>ronaldo</th>\n",
       "      <th>politica</th>\n",
       "      <th>pp</th>\n",
       "      <th>rajoy</th>\n",
       "      <th>psoe</th>\n",
       "      <th>zapatero</th>\n",
       "      <th>dinero</th>\n",
       "      <th>fmi</th>\n",
       "      <th>ibex</th>\n",
       "      <th>pib</th>\n",
       "      <th>ue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         balon  futbol  liga  messi  ronaldo  politica   pp  rajoy  psoe  \\\n",
       "Topic 1   0.09    0.28  0.08   0.15     0.16      0.05 0.05   0.07  0.01   \n",
       "Topic 2   0.01    0.02  0.01   0.01     0.01      0.23 0.20   0.11  0.19   \n",
       "Topic 3   0.01    0.01  0.01   0.01     0.01      0.05 0.02   0.04  0.09   \n",
       "\n",
       "         zapatero  dinero  fmi  ibex  pib   ue  \n",
       "Topic 1      0.02    0.01 0.01  0.01 0.02 0.01  \n",
       "Topic 2      0.16    0.01 0.01  0.01 0.01 0.01  \n",
       "Topic 3      0.09    0.16 0.22  0.04 0.07 0.19  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(words_topics, index=['Topic 1', 'Topic 2', 'Topic 3'], columns=dictionary.token2id.keys()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras (terminos) más representativas de un tema (topic)\n",
    "\n",
    "\n",
    "* Dado que podemos obtener la probabilidad de que dada una palabra (termino) esta pertenezca a un tema (topic), podemos obtener las palabras más representativas por topic de la siguiente manera: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1\n",
      "['futbol', 'ronaldo', 'messi', 'balon', 'liga']\n",
      "\n",
      "Topic 2\n",
      "['politica', 'pp', 'psoe', 'zapatero', 'rajoy']\n",
      "\n",
      "Topic 3\n",
      "['fmi', 'ue', 'dinero', 'zapatero', 'psoe']\n"
     ]
    }
   ],
   "source": [
    "dictionary.id2token\n",
    "for i in range(3):\n",
    "    print('\\nTopic {i}'.format(i=i+1))\n",
    "    pprint([dictionary.id2token[term[0]] for term in lda_model.get_topic_terms(i)[0:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Topics & Terms\n",
    "\n",
    "\n",
    "* Gensim nos devuelve un \"formula\" por tema (Topic) que aplicada a las apariciones de las palabras en los documentos nos indica la pertenencia del nuevo documento a ese tema. El que mayor valor tenga tras aplicar la fórmula del tema al documento significará que tiene mayor propensión a pertenecer a ese tema.\n",
    "\n",
    "\n",
    "* Si os fijáis esa fórmula la construye como el sumatorio de la aparición de la palabra en el documento, multiplicado por la probabilidad de que en ese temá aparezca esa palabra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.283*\"futbol\" + 0.163*\"ronaldo\" + 0.145*\"messi\" + 0.093*\"balon\" + 0.075*\"liga\" + 0.068*\"rajoy\" + 0.049*\"politica\" + 0.047*\"pp\" + 0.023*\"zapatero\" + 0.021*\"pib\" + 0.007*\"fmi\" + 0.007*\"dinero\" + 0.006*\"psoe\" + 0.006*\"ue\" + 0.006*\"ibex\"'),\n",
       " (1,\n",
       "  '0.234*\"politica\" + 0.200*\"pp\" + 0.188*\"psoe\" + 0.156*\"zapatero\" + 0.107*\"rajoy\" + 0.015*\"futbol\" + 0.014*\"pib\" + 0.012*\"ue\" + 0.011*\"fmi\" + 0.011*\"messi\" + 0.011*\"dinero\" + 0.011*\"ronaldo\" + 0.010*\"liga\" + 0.010*\"balon\" + 0.010*\"ibex\"'),\n",
       " (2,\n",
       "  '0.223*\"fmi\" + 0.190*\"ue\" + 0.156*\"dinero\" + 0.088*\"zapatero\" + 0.085*\"psoe\" + 0.073*\"pib\" + 0.051*\"politica\" + 0.040*\"rajoy\" + 0.039*\"ibex\" + 0.023*\"pp\" + 0.007*\"futbol\" + 0.006*\"ronaldo\" + 0.006*\"liga\" + 0.006*\"messi\" + 0.006*\"balon\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_words=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "# Visualización\n",
    "\n",
    "\n",
    "* Existe una librería llamada \"***pyLDAvis***\" que nos permite visualizar las relaciones entre los temas (topic) y dentro de cada tema la importancia de sus palabras (terms).\n",
    "\n",
    "\n",
    "* La parte de visualización de esta librería nos permite ver:\n",
    "    - Parte Izquierda: Visualización de los temas en función de dos componentes (2 Dimensiones)\n",
    "    - Parte Derecha: Seleccionado un Topic, podemos ver las palabras (terms) más relevantes de ese tema y la frecuencia con la que aparecen tanto en el corpus como en el tema.\n",
    "    \n",
    "    \n",
    "* En esta visualización podemos apreciar como se distinguen los tres temas claramente ya que las dos componentes que las definen son claramente distintas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el248543014566568255513485\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el248543014566568255513485_data = {\"mdsDat\": {\"x\": [-0.044026843069122586, -0.1856793631661315, 0.22970620623525403], \"y\": [-0.12875282964241858, 0.08484624228852324, 0.04390658735389523], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [34.54959499030345, 33.8285389421556, 31.62186606754095]}, \"tinfo\": {\"Term\": [\"futbol\", \"fmi\", \"ue\", \"ronaldo\", \"dinero\", \"messi\", \"psoe\", \"pp\", \"politica\", \"balon\", \"zapatero\", \"liga\", \"pib\", \"ibex\", \"rajoy\", \"pp\", \"politica\", \"psoe\", \"zapatero\", \"rajoy\", \"ibex\", \"pib\", \"liga\", \"balon\", \"messi\", \"ronaldo\", \"dinero\", \"ue\", \"futbol\", \"fmi\", \"fmi\", \"ue\", \"dinero\", \"ibex\", \"pib\", \"zapatero\", \"psoe\", \"rajoy\", \"politica\", \"pp\", \"liga\", \"balon\", \"messi\", \"ronaldo\", \"futbol\", \"futbol\", \"ronaldo\", \"messi\", \"balon\", \"liga\", \"rajoy\", \"pib\", \"pp\", \"politica\", \"ibex\", \"zapatero\", \"dinero\", \"ue\", \"fmi\", \"psoe\"], \"Freq\": [13.0, 11.0, 9.0, 7.0, 7.0, 7.0, 13.0, 12.0, 15.0, 4.0, 12.0, 4.0, 4.0, 2.0, 9.0, 9.392845106620062, 10.975638985841854, 8.839842267224576, 7.349058106896194, 5.0401981724312295, 0.4592794604051902, 0.6511107989120156, 0.4844885557068239, 0.47615271085148064, 0.5069022320352975, 0.5028254626715127, 0.506868186387816, 0.5558894114325332, 0.7110358646104787, 0.5353203265341887, 10.26928940321644, 8.723161462780824, 7.192561341711939, 1.802849583692219, 3.3582453699584716, 4.033987275491063, 3.919905791991333, 1.8298568491225689, 2.35569411198793, 1.0470149801995605, 0.2933185797142363, 0.2737771789575514, 0.28780877467006866, 0.29480456289154733, 0.32453828031401566, 12.152709623063831, 6.999767912939102, 6.247219481834601, 4.007063228371492, 3.226276063415997, 2.9443578164443047, 0.9054664249519359, 2.006829032915371, 2.1105942279758443, 0.2568071705384296, 1.0095075797715083, 0.28226107390318833, 0.2786094734392071, 0.2991211137482442, 0.27914979784125754], \"Total\": [13.0, 11.0, 9.0, 7.0, 7.0, 7.0, 13.0, 12.0, 15.0, 4.0, 12.0, 4.0, 4.0, 2.0, 9.0, 12.446689119734993, 15.44192732580563, 13.038897857057167, 12.392552962158765, 9.814412837998104, 2.518936214635839, 4.914822593822422, 4.004083198837057, 4.7569931181805245, 7.041930488539967, 7.797397938502162, 7.981690602002944, 9.557660347652565, 13.188283767988326, 11.103730843498873, 11.103730843498873, 9.557660347652565, 7.981690602002944, 2.518936214635839, 4.914822593822422, 12.392552962158765, 13.038897857057167, 9.814412837998104, 15.44192732580563, 12.446689119734993, 4.004083198837057, 4.7569931181805245, 7.041930488539967, 7.797397938502162, 13.188283767988326, 13.188283767988326, 7.797397938502162, 7.041930488539967, 4.7569931181805245, 4.004083198837057, 9.814412837998104, 4.914822593822422, 12.446689119734993, 15.44192732580563, 2.518936214635839, 12.392552962158765, 7.981690602002944, 9.557660347652565, 11.103730843498873, 13.038897857057167], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.6099, -1.4542, -1.6706, -1.8553, -2.2324, -4.628, -4.279, -4.5745, -4.5919, -4.5293, -4.5374, -4.5294, -4.4371, -4.1909, -4.4748, -1.4996, -1.6628, -1.8557, -3.2394, -2.6174, -2.434, -2.4627, -3.2246, -2.972, -3.7828, -5.0553, -5.1242, -5.0742, -5.0502, -4.9541, -1.2638, -1.8155, -1.9292, -2.3733, -2.59, -2.6814, -3.8606, -3.0648, -3.0144, -5.1208, -3.7519, -5.0263, -5.0393, -4.9682, -5.0373], \"loglift\": [15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7813, 0.7214, 0.6741, 0.5403, 0.3964, -0.6392, -0.9586, -1.0492, -1.2389, -1.5685, -1.6785, -1.6939, -1.7818, -1.8576, -1.9694, 1.0057, 0.9925, 0.9798, 0.7494, 0.703, -0.0385, -0.118, -0.5957, -0.7964, -1.3916, -1.5299, -1.7712, -2.1135, -2.1914, -2.6208, 1.0695, 1.0434, 1.0316, 0.9798, 0.9353, -0.0526, -0.5402, -0.6736, -0.8388, -1.1319, -1.3563, -2.1908, -2.384, -2.4629, -2.6926]}, \"token.table\": {\"Topic\": [3, 1, 2, 1, 2, 1, 3, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 3, 1, 2, 1, 2, 3], \"Freq\": [0.8408673085341645, 0.12528674059967418, 0.8770071841977192, 0.09005981990147846, 0.9005981990147846, 0.07582487741333571, 0.9098985289600285, 0.7939859645430278, 0.7492351809451208, 0.14200651392787805, 0.8520390835672683, 0.20346614367259724, 0.6103984310177917, 0.20346614367259724, 0.7123463132492183, 0.12951751149985785, 0.12951751149985785, 0.7230838589621352, 0.0803426509957928, 0.1606853019915856, 0.6902423884798549, 0.30677439487993546, 0.509454827561531, 0.2037819310246124, 0.3056728965369186, 0.12824791140415934, 0.8977353798291153, 0.10462811646634919, 0.9416530481971427, 0.5648553628437033, 0.3227744930535448, 0.0806936232633862], \"Term\": [\"balon\", \"dinero\", \"dinero\", \"fmi\", \"fmi\", \"futbol\", \"futbol\", \"ibex\", \"liga\", \"messi\", \"messi\", \"pib\", \"pib\", \"pib\", \"politica\", \"politica\", \"politica\", \"pp\", \"pp\", \"pp\", \"psoe\", \"psoe\", \"rajoy\", \"rajoy\", \"rajoy\", \"ronaldo\", \"ronaldo\", \"ue\", \"ue\", \"zapatero\", \"zapatero\", \"zapatero\"]}, \"R\": 15, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el248543014566568255513485\", ldavis_el248543014566568255513485_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el248543014566568255513485\", ldavis_el248543014566568255513485_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el248543014566568255513485\", ldavis_el248543014566568255513485_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
