{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso Practico 3.1\n",
    "\n",
    "\n",
    "* En este notebook vamos a resolver un problema con Aprendizaje por refuerzo usando los algoritmos del Q-Learning y SARSA-Learning (implementados en este Notebook).\n",
    "\n",
    "\n",
    "* El problema que queremos resolver es el de encontrar el camino que nos suponga una mayor recompensa (el más corto) desde un estado inicial $[0,0]$ hasta el estado final $[4,4]$, pudiendo realizar 4 tipos de acciones:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/007_RL.png\" style=\"width: 250px;\"/>\n",
    "\n",
    "\n",
    "* Como vemos en esta imagen (el entorno) tenemos dos estados en lo que podemos recibir recompensa bien sea negativa (-100) o positiva (+100).\n",
    "\n",
    "* Para resolver este problema vamos a realizar lo siguiente:\n",
    "<span></span><br>\n",
    "    1. [Definición del entorno](#M1)\n",
    "<span></span><br>\n",
    "    2. [Implementación de un algoritmo de toma aleatoria de acciones](#M2)\n",
    "<span></span><br>\n",
    "    3. [Ejecución: Entorno - Agente](#M3)\n",
    "<span></span><br>\n",
    "    4. [Q-Learner: Implementación y Ejecución](#M4)\n",
    "<span></span><br>\n",
    "    5. [SARSA-Learner: Implementación y Ejecución](#M5)\n",
    "<span></span><br>\n",
    "    6. [Ejecuciones a realizar por el alumno](#M6)\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M1\">1.- Definición del entorno</a>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerias necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Formato de los decimales en Pandas y la semilla del Random\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.random.seed(5)\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "    def __init__(self, action_penalty=-1.0):\n",
    "        \"\"\"\n",
    "        Clase que representa y controla en entorno\n",
    "        :param step_penalty: Factor de descuento del Reward por acción tomada\n",
    "        \"\"\"\n",
    "        self.actions = {'Arriba': [-1, 0],\n",
    "                        'Abajo': [1, 0],\n",
    "                        'Izquierda': [0, -1],\n",
    "                        'Derecha': [0, 1]}\n",
    "        self.rewards = [[0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, -100.0, 100.0]]\n",
    "        self.action_penalty = action_penalty\n",
    "        self.state = [0, 0]\n",
    "        self.final_state = [3, 3]\n",
    "        self.total_reward = 0.0\n",
    "        self.actions_done = []\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Método que reinicia las variables del entorno y devuelve es estado inicial\n",
    "        :return: state\n",
    "        \"\"\"\n",
    "        self.total_reward = 0.0\n",
    "        self.state = [0, 0]\n",
    "        self.actions_done = []\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Método que ejecuta una acción determinada del conjunto de acciones {Arriba, Abajo, Izquierda, Derecha}\n",
    "        para guiar al agente en el entorno.\n",
    "        :param action: Acción a ejecutar\n",
    "        :return: (state, reward, is_final_state)\n",
    "        \"\"\"\n",
    "        self.__apply_action(action)\n",
    "        self.actions_done.append(self.state[:])\n",
    "        is_final_state = np.array_equal(self.state, self.final_state)\n",
    "        reward = self.rewards[self.state[0]][self.state[1]] + self.action_penalty\n",
    "        self.total_reward += reward\n",
    "        return self.state, reward, is_final_state\n",
    "\n",
    "    def __apply_action(self, action):\n",
    "        \"\"\"\n",
    "        Método que calcula el nuevo estado a partir de la acción a ejecutar\n",
    "        :param action: Acción a ejecutar\n",
    "        \"\"\"\n",
    "        self.state[0] += self.actions[action][0]\n",
    "        self.state[1] += self.actions[action][1]\n",
    "\n",
    "        # Si me salgo del tablero por arriba o por abajo, me quedo en la posición que estaba\n",
    "        if self.state[0] < 0:\n",
    "            self.state[0] = 0\n",
    "        elif self.state[0] > len(self.rewards) - 1:\n",
    "            self.state[0] -= 1\n",
    "\n",
    "        # Si me salgo del tablero por los lados, me quedo en la posición que estaba\n",
    "        if self.state[1] < 0:\n",
    "            self.state[1] = 0\n",
    "        elif self.state[1] > len(self.rewards[0]) - 1:\n",
    "            self.state[1] -= 1\n",
    "\n",
    "    def print_path_episode(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el camino seguido por el agente\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        path = [['-' for _ in range(len(self.rewards))] for _ in range(len(self.rewards[0]))]\n",
    "        path[0][0] = '0'\n",
    "        for index, step in enumerate(self.actions_done):\n",
    "            path[step[0]][step[1]] = str(index + 1)\n",
    "\n",
    "        print(pd.DataFrame(data=np.array([np.array(xi) for xi in path]),\n",
    "                           index=[\"x{}\".format(str(i)) for i in range(len(path))],\n",
    "                           columns=[\"y{}\".format(str(i)) for i in range(len(path[0]))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M2\">2.- Implementación de un algoritmo de toma aleatoria de acciones</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "\n",
    "    def __init__(self, environment, learning_rate=0.1, discount_factor=0.1, ratio_exploration=0.05):\n",
    "        \"\"\"\n",
    "        Clase que implementa un algoritmo de aprendiza por refuerzo\n",
    "        Esta clase implementa un algoritmo de selección aleatoria de acciones\n",
    "        :param environment: Entorno en el que tomar las acciones\n",
    "        :param learning_rate: Factor de aprendizaje\n",
    "        :param discount_factor: Factor de descuento (0=Estrategia a corto plazo, 1=Estrategia a largo plazo)\n",
    "        :param ratio_exploration: Ratio de exploración\n",
    "        \"\"\"\n",
    "        self.environment = environment\n",
    "        self.q_table = [[[0.0 for _ in self.environment.actions]\n",
    "                         for _ in range(len(self.environment.rewards))]\n",
    "                        for _ in range(len(self.environment.rewards[0]))]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.ratio_exploration = ratio_exploration\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'random'\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        \"\"\"\n",
    "        Método que selecciona la siguiente acción a tomar:\n",
    "            Aleatoria -> si el ratio de exploración es inferior al umbral\n",
    "            Mejor Acción -> si el ratio de exploración es superior al umbral\n",
    "        :param state: Estado del agente\n",
    "        :return: next_action\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.uniform() < self.ratio_exploration:\n",
    "            # Selecciono una opción al azar\n",
    "            next_action = np.random.choice(list(self.environment.actions))\n",
    "        else:\n",
    "            # Selecciono la acción que me de mayor valor. Si hay empate, selecciono una al azar\n",
    "            idx_action = np.random.choice(np.flatnonzero(\n",
    "                self.q_table[state[0]][state[1]] == np.array(self.q_table[state[0]][state[1]]).max()\n",
    "            ))\n",
    "            next_action = list(self.environment.actions)[idx_action]\n",
    "\n",
    "        return next_action\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Actualiza la Q-Table\n",
    "        :param kwargs: \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def print_q_table(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla la Q-Table\n",
    "        \"\"\"\n",
    "        print(\"Actions = {}\".format(' - '.join([k for k, v in self.environment.actions.items()])))\n",
    "        for row in self.q_table:\n",
    "            for col in row:\n",
    "                print('[', end='')\n",
    "                for val in col:\n",
    "                    print('{:0.2f}'.format(val), end=' ')\n",
    "                print('] ', end='')\n",
    "            print()\n",
    "\n",
    "    def print_best_values_states(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el valor de la mejor opción a realizar en cada uno de los estados\n",
    "        \"\"\"\n",
    "        best = [[max(vi) for vi in row] for row in self.q_table]\n",
    "        print(pd.DataFrame(data=np.array([np.array(xi) for xi in best]),\n",
    "                           index=[\"x{}\".format(str(i)) for i in range(len(best))],\n",
    "                           columns=[\"y{}\".format(str(i)) for i in range(len(best[0]))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M3\">3.- Ejecución: Entorno - Agente</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_agent(learner=Learner, num_episodes=10, learning_rate=0.1, discount_factor=0.1, ratio_exploration=0.05,\n",
    "              verbose=False):\n",
    "    \"\"\"\n",
    "    Método que ejecuta el proceso de aprendizaje del agente en un entorno\n",
    "    :param learner: Algoritmo de Aprendizaje\n",
    "    :param num_episodes: Número de veces que se ejecuta (o aprende) el agente en el entorno\n",
    "    :param learning_rate: Factor de Aprendizaje\n",
    "    :param discount_factor: Factor de descuento (0=Estrategia a corto plazo, 1=Estrategia a largo plazo)\n",
    "    :param ratio_exploration: Ratio de exploración\n",
    "    :param verbose: Boolean, si queremos o no imprimir por pantalla información del proceso\n",
    "    :return: (episodes_list, best_episode)\n",
    "    \"\"\"\n",
    "\n",
    "    # Instanciamos el entorno\n",
    "    environment = Environment()\n",
    "\n",
    "    # Instanciamos el método de aprendizaje\n",
    "    learner = learner(environment=environment,\n",
    "                      learning_rate=learning_rate,\n",
    "                      discount_factor=discount_factor,\n",
    "                      ratio_exploration=ratio_exploration)\n",
    "\n",
    "    # Variables para guardar la información de los episodios\n",
    "    episodes_list = []\n",
    "    best_reward = float('-inf')\n",
    "    best_episode = None\n",
    "\n",
    "    for n_episode in range(0, num_episodes):\n",
    "        state = environment.reset()\n",
    "        reward = None\n",
    "        is_final_state = None\n",
    "        num_steps_episode = 0\n",
    "        while (is_final_state != True):\n",
    "            old_state = state[:]\n",
    "            next_action = learner.get_next_action(state=state)\n",
    "            state, reward, is_final_state = environment.step(next_action)\n",
    "            next_post_action = learner.get_next_action(state) if learner.name == 'SARSA' else None\n",
    "            learner.update(environment=environment,\n",
    "                           old_state=old_state,\n",
    "                           action_taken=next_action,\n",
    "                           reward_action_taken=reward,\n",
    "                           new_state=state,\n",
    "                           new_action=next_post_action,\n",
    "                           is_final_state=is_final_state)\n",
    "            num_steps_episode += 1\n",
    "\n",
    "        # Guardamos la información del episodio\n",
    "        episodes_list.append([n_episode + 1, num_steps_episode, environment.total_reward])\n",
    "\n",
    "        # Guardamos el mejor episodio\n",
    "        if environment.total_reward >= best_reward:\n",
    "            best_reward = environment.total_reward\n",
    "            best_episode = {'num_episode': n_episode + 1,\n",
    "                            'episode': deepcopy(environment),\n",
    "                            'learner': deepcopy(learner)}\n",
    "\n",
    "        if verbose:\n",
    "            # Imprimimos la información de los episodios\n",
    "            print('EPISODIO {} - Numero de acciones: {} - Reward: {}'\n",
    "                  .format(n_episode + 1, num_steps_episode, environment.total_reward))\n",
    "\n",
    "    return episodes_list, best_episode\n",
    "\n",
    "\n",
    "def print_process_info(episodes_list, best_episode, print_best_episode_info=True,\n",
    "                       print_q_table=True, print_best_values_states=True,\n",
    "                       print_steps=True, print_path=True):\n",
    "    \"\"\"\n",
    "    Método que imprime por pantalla los resultados de la ejecución\n",
    "    \"\"\"\n",
    "    if print_best_episode_info:\n",
    "        print('\\nMEJOR (ÚLTIMO) EPISODIO:\\n\\EPISODIO {}\\n\\tNumero de acciones: {}\\n\\tReward: {}'\n",
    "              .format(best_episode['num_episode'],\n",
    "                      len(best_episode['episode'].actions_done),\n",
    "                      best_episode['episode'].total_reward))\n",
    "\n",
    "    if print_q_table:\n",
    "        print('\\nQ_TABLE:')\n",
    "        best_episode['learner'].print_q_table()\n",
    "\n",
    "    if print_best_values_states:\n",
    "        print('\\nBEST Q_TABLE VALUES:')\n",
    "        best_episode['learner'].print_best_values_states()\n",
    "\n",
    "    if print_steps:\n",
    "        print('\\nPasos: \\n   {}'.format(best_episode['episode'].actions_done))\n",
    "\n",
    "    if print_path:\n",
    "        print('\\nPATH:')\n",
    "        best_episode['episode'].print_path_episode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución:\n",
    "\n",
    "* En este primer ejemplo de toma de decisiones en el que se usa una política aleatoria, podemos observar como el agente no aprende nada y los movimientos son aleatorios en los 10 episodios que ejecutamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 91 - Reward: -391.0\n",
      "EPISODIO 2 - Numero de acciones: 19 - Reward: 81.0\n",
      "EPISODIO 3 - Numero de acciones: 17 - Reward: -117.0\n",
      "EPISODIO 4 - Numero de acciones: 60 - Reward: -360.0\n",
      "EPISODIO 5 - Numero de acciones: 312 - Reward: -2012.0\n",
      "EPISODIO 6 - Numero de acciones: 148 - Reward: -648.0\n",
      "EPISODIO 7 - Numero de acciones: 19 - Reward: 81.0\n",
      "EPISODIO 8 - Numero de acciones: 216 - Reward: -616.0\n",
      "EPISODIO 9 - Numero de acciones: 52 - Reward: 48.0\n",
      "EPISODIO 10 - Numero de acciones: 45 - Reward: 55.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 7\n",
      "\tNumero de acciones: 19\n",
      "\tReward: 81.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] \n",
      "[0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] \n",
      "[0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] \n",
      "[0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "     y0   y1   y2   y3\n",
      "x0 0.00 0.00 0.00 0.00\n",
      "x1 0.00 0.00 0.00 0.00\n",
      "x2 0.00 0.00 0.00 0.00\n",
      "x3 0.00 0.00 0.00 0.00\n",
      "\n",
      "Pasos: \n",
      "   [[1, 0], [1, 1], [1, 0], [0, 0], [1, 0], [1, 0], [1, 1], [1, 2], [2, 2], [2, 3], [1, 3], [2, 3], [2, 2], [2, 3], [2, 2], [2, 1], [2, 2], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0  y1  y2  y3\n",
      "x0  4   -   -   -\n",
      "x1  6   7   8  11\n",
      "x2  -  16  17  18\n",
      "x3  -   -   -  19\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(learner=Learner, \n",
    "                                        verbose=True)\n",
    "\n",
    "print_process_info(episodes_list=episodes_list, \n",
    "                   best_episode=best_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M4\">4.- Q-Learner: Implementación y Ejecución</a>\n",
    "\n",
    "\n",
    "* Recordemos el Pseudocódigo del Algoritmo:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/013_qlearning.png\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner(Learner):\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'QLearner'\n",
    "\n",
    "    def update(self, environment, old_state, action_taken, reward_action_taken, new_state, is_final_state, **kwargs):\n",
    "        \"\"\"\n",
    "        Método que implementa el Algoritmo de aprendizaje del Q-Learning\n",
    "        :param environment: Entorno en el que tomar las acciones\n",
    "        :param old_state: Estado actual\n",
    "        :param action_taken: Acción a realizar\n",
    "        :param reward_action_taken: Recompensa obtenida por la acción tomada\n",
    "        :param new_state: Nuevo estado al que se mueve el agente\n",
    "        :param is_final_state: Boolean. Devuelvel True si el agente llega al estado final; si no, False\n",
    "        :param kwargs: \n",
    "        \"\"\"\n",
    "        # Obtengo el identificador de la acción\n",
    "        idx_action_taken = list(environment.actions).index(action_taken)\n",
    "\n",
    "        # Obtengo el valor de la acción tomada\n",
    "        actual_q_value_options = self.q_table[old_state[0]][old_state[1]]\n",
    "        actual_q_value = actual_q_value_options[idx_action_taken]\n",
    "\n",
    "        future_q_value_options = self.q_table[new_state[0]][new_state[1]]\n",
    "        future_max_q_value = reward_action_taken + self.discount_factor * max(future_q_value_options)\n",
    "        if is_final_state:\n",
    "            # Reward máximo si llego a la posición final\n",
    "            future_max_q_value = reward_action_taken\n",
    "\n",
    "        self.q_table[old_state[0]][old_state[1]][idx_action_taken] = \\\n",
    "            actual_q_value + self.learning_rate * (future_max_q_value - actual_q_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución con una estrategia a corto plazo\n",
    "\n",
    "* En esta ejecución podemos ver como el agente aprende a interactuar con el entorno siguiendo una estrategia a corto plazo (discount_factor=0.1) pero los movimientos los realiza hacia el estado que mayor recompensa parcial le de y no tiene muy encuenta la recompensa final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 41 - Reward: -41.0\n",
      "EPISODIO 2 - Numero de acciones: 52 - Reward: -52.0\n",
      "EPISODIO 3 - Numero de acciones: 29 - Reward: 71.0\n",
      "EPISODIO 4 - Numero de acciones: 18 - Reward: 82.0\n",
      "EPISODIO 5 - Numero de acciones: 35 - Reward: 65.0\n",
      "EPISODIO 6 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 7 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 8 - Numero de acciones: 13 - Reward: 87.0\n",
      "EPISODIO 9 - Numero de acciones: 43 - Reward: 57.0\n",
      "EPISODIO 10 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 11 - Numero de acciones: 21 - Reward: -21.0\n",
      "EPISODIO 12 - Numero de acciones: 11 - Reward: 89.0\n",
      "EPISODIO 13 - Numero de acciones: 24 - Reward: 76.0\n",
      "EPISODIO 14 - Numero de acciones: 36 - Reward: 64.0\n",
      "EPISODIO 15 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 16 - Numero de acciones: 41 - Reward: 59.0\n",
      "EPISODIO 17 - Numero de acciones: 17 - Reward: 83.0\n",
      "EPISODIO 18 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 19 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 20 - Numero de acciones: 43 - Reward: 57.0\n",
      "EPISODIO 21 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 22 - Numero de acciones: 17 - Reward: 83.0\n",
      "EPISODIO 23 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 24 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 25 - Numero de acciones: 28 - Reward: 72.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 23\n",
      "\tNumero de acciones: 6\n",
      "\tReward: 94.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[-1.00 -1.00 -1.00 -1.00 ] [-0.81 -0.83 -0.83 -0.83 ] [-0.59 -0.58 -0.60 -0.58 ] [-0.35 -0.38 -0.35 -0.42 ] \n",
      "[-0.86 -0.85 -0.84 -0.83 ] [-0.69 -0.71 -0.69 -0.70 ] [-0.42 -0.42 -0.43 -0.42 ] [-0.10 3.11 -0.10 -0.10 ] \n",
      "[-0.69 -0.68 -0.68 -0.71 ] [-0.54 -0.54 -0.54 -0.52 ] [-0.19 -10.10 -0.10 4.45 ] [-0.10 89.25 0.00 -0.10 ] \n",
      "[-0.59 -0.59 -0.59 -0.63 ] [-0.59 -0.54 -0.54 -19.19 ] [-0.10 0.00 -0.10 9.90 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0 -1.00 -0.81 -0.58 -0.35\n",
      "x1 -0.83 -0.69 -0.42  3.11\n",
      "x2 -0.68 -0.52  4.45 89.25\n",
      "x3 -0.59 -0.54  9.90  0.00\n",
      "\n",
      "Pasos: \n",
      "   [[1, 0], [2, 0], [2, 1], [2, 2], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  -  -  -\n",
      "x1  1  -  -  -\n",
      "x2  2  3  4  5\n",
      "x3  -  -  -  6\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(learner=QLearner,\n",
    "                                        num_episodes=25,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.1,\n",
    "                                        ratio_exploration=0.05,\n",
    "                                        verbose=True)\n",
    "\n",
    "print_process_info(episodes_list=episodes_list,\n",
    "                   best_episode=best_episode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución con una estrategia a largo plazo\n",
    "\n",
    "* En esta ejecución podemos ver como el agente aprende a interactuar con el entorno siguiendo una estrategia a largo plazo (discount_factor=0.9), realizando movimientos que le den una recompesa final mayor y no asi una recompensa parcial mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 20 - Reward: -120.0\n",
      "EPISODIO 2 - Numero de acciones: 41 - Reward: 59.0\n",
      "EPISODIO 3 - Numero de acciones: 21 - Reward: 79.0\n",
      "EPISODIO 4 - Numero de acciones: 54 - Reward: 46.0\n",
      "EPISODIO 5 - Numero de acciones: 23 - Reward: 77.0\n",
      "EPISODIO 6 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 7 - Numero de acciones: 17 - Reward: 83.0\n",
      "EPISODIO 8 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 9 - Numero de acciones: 19 - Reward: 81.0\n",
      "EPISODIO 10 - Numero de acciones: 11 - Reward: 89.0\n",
      "EPISODIO 11 - Numero de acciones: 16 - Reward: 84.0\n",
      "EPISODIO 12 - Numero de acciones: 11 - Reward: 89.0\n",
      "EPISODIO 13 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 14 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 15 - Numero de acciones: 13 - Reward: 87.0\n",
      "EPISODIO 16 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 17 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 18 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 19 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 20 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 21 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 22 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 23 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 24 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 25 - Numero de acciones: 6 - Reward: 94.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 25\n",
      "\tNumero de acciones: 6\n",
      "\tReward: 94.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[-1.22 -0.81 -1.39 -1.21 ] [-0.77 -0.48 -0.80 -0.75 ] [-0.39 0.30 -0.44 -0.37 ] [-0.20 -0.14 -0.20 -0.20 ] \n",
      "[-0.79 -0.69 -0.77 2.46 ] [-0.51 -0.45 -0.51 12.08 ] [-0.30 32.14 -0.31 -0.19 ] [-0.10 4.32 -0.11 -0.10 ] \n",
      "[-0.43 -0.49 -0.39 -0.41 ] [-0.41 -0.38 -0.21 1.03 ] [-0.10 -10.10 -0.10 61.60 ] [0.00 91.10 0.00 10.09 ] \n",
      "[-0.40 -0.39 -0.39 -0.48 ] [-0.29 -0.39 -0.39 -10.10 ] [-0.10 0.00 0.00 9.90 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0 -0.81 -0.48  0.30 -0.14\n",
      "x1  2.46 12.08 32.14  4.32\n",
      "x2 -0.39  1.03 61.60 91.10\n",
      "x3 -0.39 -0.29  9.90  0.00\n",
      "\n",
      "Pasos: \n",
      "   [[1, 0], [1, 1], [1, 2], [2, 2], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  -  -  -\n",
      "x1  1  2  3  -\n",
      "x2  -  -  4  5\n",
      "x3  -  -  -  6\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(learner=QLearner,\n",
    "                                        num_episodes=25,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.9,\n",
    "                                        ratio_exploration=0.05,\n",
    "                                        verbose=True)\n",
    "\n",
    "print_process_info(episodes_list=episodes_list,\n",
    "                   best_episode=best_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M5\">5.- SARSA-Learner: Implementación y Ejecución</a>\n",
    "\n",
    "\n",
    "* Recordemos el Pseudocódigo del Algoritmo:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/014_sarsa.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSALearner(Learner):\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'SARSA'\n",
    "\n",
    "    def update(self, environment, old_state, action_taken, reward_action_taken, new_state, new_action, is_final_state):\n",
    "        \"\"\"\n",
    "        Método que implementa el algoritmo de aprendizaje SARSA\n",
    "        :param environment: Entorno en el que tomar las acciones\n",
    "        :param old_state: Estado actual\n",
    "        :param action_taken: Acción a realizar\n",
    "        :param reward_action_taken: Recompensa obtenida por la acción tomada\n",
    "        :param new_state: Nuevo estado al que se mueve el agente \n",
    "        :param new_action: Acción a tomar en el nuevo estado\n",
    "        :param is_final_state: Boolean. Devuelvel True si el agente llega al estado final; si no, False \n",
    "        \"\"\"\n",
    "        # Obtengo el identificador de la acción\n",
    "        idx_action_taken = list(environment.actions).index(action_taken)\n",
    "\n",
    "        # Obtengo el valor de la acción tomada\n",
    "        actual_q_value_options = self.q_table[old_state[0]][old_state[1]]\n",
    "        actual_q_value = actual_q_value_options[idx_action_taken]\n",
    "\n",
    "        future_q_value_options = self.q_table[new_state[0]][new_state[1]]\n",
    "\n",
    "        idx_new_action_taken = list(environment.actions).index(new_action)\n",
    "        future_new_action_q_value = \\\n",
    "            reward_action_taken + self.discount_factor * future_q_value_options[idx_new_action_taken]\n",
    "        if is_final_state:\n",
    "            # Reward máximo si llego a la posición final\n",
    "            future_new_action_q_value = reward_action_taken  # reward máximo\n",
    "\n",
    "        self.q_table[old_state[0]][old_state[1]][idx_action_taken] = \\\n",
    "            actual_q_value + self.learning_rate * (future_new_action_q_value - actual_q_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución con una estrategia a corto plazo\n",
    "\n",
    "* En esta ejecución podemos ver como el agente aprende a interactuar con el entorno siguiendo una estrategia a corto plazo (discount_factor=0.1) pero los movimientos los realiza hacia el estado que mayor recompensa parcial le de y no tiene muy encuenta la recompensa final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 115 - Reward: -315.0\n",
      "EPISODIO 2 - Numero de acciones: 28 - Reward: 72.0\n",
      "EPISODIO 3 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 4 - Numero de acciones: 16 - Reward: 84.0\n",
      "EPISODIO 5 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 6 - Numero de acciones: 40 - Reward: 60.0\n",
      "EPISODIO 7 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 8 - Numero de acciones: 19 - Reward: 81.0\n",
      "EPISODIO 9 - Numero de acciones: 16 - Reward: 84.0\n",
      "EPISODIO 10 - Numero de acciones: 50 - Reward: 50.0\n",
      "EPISODIO 11 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 12 - Numero de acciones: 28 - Reward: 72.0\n",
      "EPISODIO 13 - Numero de acciones: 17 - Reward: 83.0\n",
      "EPISODIO 14 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 15 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 16 - Numero de acciones: 50 - Reward: 50.0\n",
      "EPISODIO 17 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 18 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 19 - Numero de acciones: 30 - Reward: 70.0\n",
      "EPISODIO 20 - Numero de acciones: 11 - Reward: 89.0\n",
      "EPISODIO 21 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 22 - Numero de acciones: 40 - Reward: 60.0\n",
      "EPISODIO 23 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 24 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 25 - Numero de acciones: 22 - Reward: 78.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 3\n",
      "\tNumero de acciones: 7\n",
      "\tReward: 93.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[-0.35 -0.35 -0.35 -0.42 ] [-0.27 -0.35 -0.28 -0.27 ] [-0.19 -0.19 -0.19 -0.27 ] [-0.19 -0.27 -0.19 -0.19 ] \n",
      "[-0.27 -0.27 -0.27 -0.27 ] [-0.27 -0.27 -0.19 -0.27 ] [-0.19 -0.19 -0.19 -0.27 ] [-0.19 -0.08 -0.19 -0.19 ] \n",
      "[-0.27 -0.27 -0.27 -0.27 ] [-0.19 -0.27 -0.27 -0.27 ] [-0.19 -10.10 -0.19 -0.09 ] [-0.10 26.83 -0.10 -0.10 ] \n",
      "[-0.27 -0.27 -0.27 -0.27 ] [-0.27 -0.27 -0.27 -10.10 ] [-0.10 -10.10 -0.10 0.00 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0 -0.35 -0.27 -0.19 -0.19\n",
      "x1 -0.27 -0.19 -0.19 -0.08\n",
      "x2 -0.27 -0.19 -0.09 26.83\n",
      "x3 -0.27 -0.27  0.00  0.00\n",
      "\n",
      "Pasos: \n",
      "   [[0, 1], [1, 1], [1, 2], [1, 3], [1, 3], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  1  -  -\n",
      "x1  -  2  3  5\n",
      "x2  -  -  -  6\n",
      "x3  -  -  -  7\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(learner=SARSALearner,\n",
    "                                        num_episodes=25,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.1,\n",
    "                                        ratio_exploration=0.05,\n",
    "                                        verbose=True)\n",
    "\n",
    "print_process_info(episodes_list=episodes_list,\n",
    "                   best_episode=best_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución con una estrategia a largo plazo\n",
    "\n",
    "* En esta ejecución podemos ver como el agente aprende a interactuar con el entorno siguiendo una estrategia a largo plazo (discount_factor=0.9), realizando movimientos que le den una recompesa final mayor y no asi una recompensa parcial mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 23 - Reward: -223.0\n",
      "EPISODIO 2 - Numero de acciones: 34 - Reward: 66.0\n",
      "EPISODIO 3 - Numero de acciones: 50 - Reward: 50.0\n",
      "EPISODIO 4 - Numero de acciones: 35 - Reward: 65.0\n",
      "EPISODIO 5 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 6 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 7 - Numero de acciones: 31 - Reward: 69.0\n",
      "EPISODIO 8 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 9 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 10 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 11 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 12 - Numero de acciones: 17 - Reward: 83.0\n",
      "EPISODIO 13 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 14 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 15 - Numero de acciones: 9 - Reward: 91.0\n",
      "EPISODIO 16 - Numero de acciones: 16 - Reward: 84.0\n",
      "EPISODIO 17 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 18 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 19 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 20 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 21 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 22 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 23 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 24 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 25 - Numero de acciones: 6 - Reward: 94.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 25\n",
      "\tNumero de acciones: 6\n",
      "\tReward: 94.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[-1.32 -1.29 -1.31 -1.32 ] [-0.77 -0.16 -0.80 -0.75 ] [-0.39 0.32 -0.44 -0.44 ] [-0.30 0.28 -0.30 -0.30 ] \n",
      "[-0.81 -0.77 -0.77 -0.42 ] [-0.40 -0.02 -0.43 5.80 ] [-0.21 25.33 -0.22 -0.28 ] [-0.20 10.17 -0.11 0.17 ] \n",
      "[-0.53 -0.47 -0.49 -0.30 ] [-0.31 -0.30 -0.31 7.65 ] [-0.10 -10.10 0.57 60.56 ] [0.00 91.10 -0.10 0.00 ] \n",
      "[-0.39 -0.30 -0.31 -0.39 ] [-0.39 -0.30 -0.29 -10.10 ] [0.00 -10.10 -0.10 9.90 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0 -1.29 -0.16  0.32  0.28\n",
      "x1 -0.42  5.80 25.33 10.17\n",
      "x2 -0.30  7.65 60.56 91.10\n",
      "x3 -0.30 -0.29  9.90  0.00\n",
      "\n",
      "Pasos: \n",
      "   [[0, 1], [1, 1], [1, 2], [2, 2], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  1  -  -\n",
      "x1  -  2  3  -\n",
      "x2  -  -  4  5\n",
      "x3  -  -  -  6\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(learner=SARSALearner,\n",
    "                                        num_episodes=25,\n",
    "                                        learning_rate=0.1,\n",
    "                                        discount_factor=0.9,\n",
    "                                        ratio_exploration=0.05,\n",
    "                                        verbose=True)\n",
    "\n",
    "print_process_info(episodes_list=episodes_list,\n",
    "                   best_episode=best_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M6\">6.- Ejecuciones a realizar por el alumno:</a>\n",
    "\n",
    "\n",
    "* Se piden realizar las siguientes 2 ejecuciones:\n",
    "\n",
    "    1. Ejecutar con el algoritmo de Q-Learning o SARSA (uno de los dos) 30 episodios con un ratio de exploración del 95% y un factor de descuento de 0.3.\n",
    "    \n",
    "    2. Ejecutar con el algoritmo de Q-Learning o SARSA (uno de los dos) 150 episodios con un ratio de exploración del 5%, un factor de descuento de 0.9 y un ratio de aprendizaje del 1% (learning_rate=0.01).\n",
    "    \n",
    "    \n",
    "* Para ambas ejecuciones se pide justifucar los resultados obtenidos en el notebook.\n",
    "\n",
    "    ***1: JUSTIFICACIÓN:*** A completar por el alumno\n",
    "    \n",
    "    ***2: JUSTIFICACIÓN:*** A completar por el alumno\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "#### 1. Ejecutar con el algoritmo de Q-Learning o SARSA (uno de los dos) 30 episodios con un ratio de exploración del 95% y un factor de descuento de 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### 2. Ejecutar con el algoritmo de Q-Learning o SARSA (uno de los dos) 150 episodios con un ratio de exploración del 5%, un factor de descuento de 0.9 y un ratio de aprendizaje del 1% (learning_rate=0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
